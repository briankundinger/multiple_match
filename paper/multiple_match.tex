\documentclass[12pt,letterpaper]{article}

\usepackage[hypertexnames=false]{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}

% Custom packages
\usepackage{amssymb}
\usepackage{booktabs}       % professional-quality tables
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}
\usepackage[algo2e]{algorithm2e} 
\usepackage{multirow}
\usepackage{sectsty}
\usepackage{tabularx}
\usepackage{tikz}           % vector graphics
\usepackage{bm}             % bold math symbols
\usepackage{xcolor}
\usepackage{microtype}
\usepackage{import}
\usepackage{titling}
\usepackage{natbib}
\usetikzlibrary{arrows, backgrounds, patterns, matrix, shapes, fit, 
  calc, shadows, plotmarks}


% Custom commands
\let\oldvec\vec
\renewcommand\vec{\bm}
\newcommand{\simfn}{\mathtt{sim}} % similarity function
\newcommand{\truncsimfn}{\underline{\simfn}} % truncated similarity function
\newcommand{\blockfn}{\mathtt{BlockFn}} % blocking function
\newcommand{\distfn}{\mathtt{dist}} % distance function
\newcommand{\valset}{\mathcal{V}} % attribute value set
\newcommand{\entset}{\mathcal{R}} % set of records that make up an entity
\newcommand{\partset}{\mathcal{E}} % set of entities that make up a partition
\newcommand{\1}[1]{\mathbb{I}\!\left[#1\right]} % indicator function
\newcommand{\euler}{\mathrm{e}} % Euler's constant
\newcommand{\dblink}{\texttt{\upshape \lowercase{d-blink}}} % Name of scalable Bayesian ER model
\newcommand{\blink}{\texttt{\upshape \lowercase{blink}}} % Name of original Bayesian ER model
\def\spacingset#1{\renewcommand{\baselinestretch}%
  {#1}\small\normalsize} \spacingset{1}

\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\def \brian#1{{\color{red} (#1)}}
\def \serge#1{{\color{blue} (#1)}}
\def \beka#1{{\color{green} (#1)}}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\sectionfont{\large\nohang\centering\MakeUppercase}

\title{Bayesian Matching with Relaxed Duplication Assumptions through Dirchlet Record Linkage}
\author{Brian Kundinger}

\begin{document}
\maketitle
%
%\bigskip
\begin{abstract}
	Probabilistic record linkage is the use of statistical methods to identify unique entities within and across databases. Bayesian methods can adopt different prior distributions for various assumptions on the nature of record duplications in the linkage task, but become computationally expensive as the size of the linkage task grows. In this paper, we propose Dirichlet Record Linkage (\texttt{DRL}), a method for linking a duplicate-free reference file to a target file that may have internal duplications. We use a Dirichlet process prior for the parameter governing the number of matches a record in the reference file has in the target file. We use the sequence of parameters in stick breaking representation of the Dirichlet process to produce a computationally efficient Gibbs sampler to conduct the linkage task. We demonstrate the speed and accuracy of our approach relative to other recent Bayesian methods through simulations and case studies. In particular, we show that \texttt{DRL} exhibits strong performance using default, non-informative hyperparameters.  
	
	\brian{Beka commented that I should mention the loss function in the abstract and introduction. To me, the loss function is not innovative enough to really call attention to it; it is used implicitly in many classification tasks. Thoughts?}
\end{abstract} 

%
%\newpage
\spacingset{1.5}
\newpage

\section{Introduction}\label{sec:introduction}

%We have shown in previous work that the fast Beta linkage framework tends to outperform standard Fellegi Sunter when each record in $B$ has at most one match in $A$. However, significant problems arise when a record in $B$ has multiple matching records in $A$. In particular, since matching probability is normalized among all records in $A$, matching probability is split among multiple records such that none of the matches achieves high enough posterior probability to be identified through the Bayes estimate. This amounts to a paradox: the more matches that record $B_j$ has in $A$, the less likely the algorithm is to find a match. 

%We attempt to resolve this paradox by extending the fabl framework to handle these internal duplications. We introduce a modified Dirichlet process prior that allows each record in $B$ to match to potentially multiple records in $A$. We emphasize that we are not interested in deduplication within $A$ for its own sake, but rather aim to conduct record linkage in light the problems posed by these duplications. Therefore, duplications within files that have no duplications across files will go undetected; we believe this acceptable for many practical applications. 

 %For this reason, we do not enforce transitive closure throughout the Gibbs sampler as done in \cite{marchant_distributed_2019} and \cite{aleshinguendel2021multifile}, but rather assume it, and create a coherent set of linkages through post-processing. This allows for a record linkage model that is robust to internal duplications while maintaining the computational advantages of the original \texttt{fabl} model.

%There have been several other attempts to estimate more flexible linkage structures within the comparison vector framework, but each has significant drawbacks in practice. First, the original \cite{fellegi_theory_1969} method that models the linkage status of each record pair as independent typically results in one record in one file being matched with multiple records in the other file. This initial estimate of the linkage structure is usually refined through post-processing to create a bipartite matching \cite{jaro1989}. As of yet, this post-processing has not been adapted to estimate more general linkage structures, and the initial estimate without any post-processing is known to have poor precision. \cite{aleshin2023multifile} allowed for multiple matches within and across datafiles, but requires that the modeller explicitly set a maximum linkage cluster size, and models the dependency between records in such a way that limits scalability. 
Probabilistic record linkage uses statistical methods to identify matching records within and across databases, generally without the use of unique identifiers. This is an increasingly important task in ``data cleaning,'' either for its own sake, or as a preliminary step before conducting subsequent analysis. This problem is increasingly important in health care \citep{gutman_bayesian_2013, hof2017probabilistic}, governmental statistics \citep{Jaro1995, Fortinietal01}, human rights statistics \citep{lum2013applications, price2015documents, Sadosky2015, sadinle_detecting_2014, sadinle_bayesian_2018}, author name disambiguation \citep{lai_2011, zhang2018name}, and forensic science \citep{Tai2019}. In this paper, we propose a method for conducting record linkage between a duplicate free reference file and a reference file that may have internal duplications. \brian{Is "internal duplications" unclear?" I could just say "duplications"?}

%It is common for data collectors to have one list of records with a rigorous sampling mechanism trusted to have no internal duplications, or have to cleaned a list over time such 

Many probabilistic record linkage methods are based on the foundational work of \cite{fellegi_theory_1969}. In these Fellegi-Sunter (FS) methods, the data analyst first creates comparison vectors for each pair of records in the data files. These vectors indicate how similar the records are on a set of variables measured in both files, known as the linkage variables.  Using these comparison vectors, the analyst classifies each pair as a match or non-match. This apprach has been extended for a wide variety of applications \citep{fair2004generalized, winkler_state_1999, wagner2014person, gill2003english}.  An alternative paradigm is to model the linkage variables directly \citep[e.g.,][]{Tancredi_2011, steorts_bayesian_2016, marchant_d-blink_2019, betancourt2022prior}. In this article, we build on the contributions to the comparison vector approach.

In the original FS formulation, the matching status of each record pair is considered independent from the matching status of all other record pairs. This can be suboptimal when we know that each file is internally free of duplications or when we desire a set of declared matches that respects transitivity. These concerns can be addressed through a Bayesian framework through different prior distributions on the linkage structure to formalize different assumptions on the type of matchings between files. For example, \cite{sadinle_bayesian_2017} proposed a prior distribution on the one-to-one matchings between two files, and \cite{aleshin2023multifile} proposed a prior on the partitions of records corresponding to unique entities found in arbitrarily many files. These more tailored record linkage methods have the advantage of producing coherent sets of matches that respect transitivity without the need for post-processing, and are generally more accurate than the standard FS approach. However, they also face limited scalability due to the computation required to model more complex dependencies between matching statuses. 

%However, these models attain this accuracy through modeling more complex dependencies between records, and thus have limited scalability. 

Given these challenges, much work has been done to increase the speed and scalability of FS methods. One approach is to use probabilistic or deterministic methods to reduce the number of record pairs considered as possible matches. With ``blocking,'' the analyst uses some feature in the data to allocate records into smaller partitions (or blocks), and runs a linkage algorithm independently on each block \citep{christen_data_2012}. Blocking on an unreliable field, however, can lead to missed matches, making this form of blocking often undesirable \citep{steorts_comparison_2014}. ``Filtering,'' is a similar technique which uses some deterministic or probabilistic criteria to set the match probability for certain comparison vectors to zero after they have been created \citep{murray2016probabilistic, mcveigh2019scaling}. Without reducing the number of comparison vectors in the model, \cite{enamorado_using_2019} used a lower dimensional representation of the set of comparison vectors that allowed for faster parameter estimation of the standard FS model. \cite{kundinger_2023} then adopted this approach for a faster implementation of the bipartite model of \cite{sadinle_bayesian_2017}. \cite{kundinger_2024_vabl} then used variational inference for further computational gains.
 
In this work, we continue to build on the these advances and provide a scalable record linkage method without the stringent assumption that each file is free of duplicates. We propose Dirichlet Record Linkage (\texttt{DRL}), an extension of the comparison vector record linkage framework for the scenario of linking one duplicate-free reference file to another file with unknown amounts of internal duplications. To do so, we model the parameter governing the amount of matches between files as a Dirichlet process. Since the resulting set of possible matches for each record is extremely high dimensional, we propose a Gibbs sampler utilizing the stick-breaking representation of the Dirichlet process for tractable posterior inference. We adopt the dimension reduction techniques of \cite{kundinger_2023} for additional gains in speed and scalability. Through simulation studies, we show that this method is able to link a reference file to a target file with various amounts of internal duplications without making any distributional assumptions on the nature of these duplications. Through a case study of voter registration records, we show that this method scalable to a linkage task involving 400,000 records without the use of blocking. 

In what follows, Section~\ref{sec:prior-work} reviews the work of \cite{fellegi_theory_1969} and a selection of Bayesian extensions to contextualize our contribution. Section~\ref{sec:drl} introduces \texttt{DRL}, and describes our method for efficient posterior sampling. Sections~\ref{sec:simulations} and \ref{sec:case-study} demonstrate the speed and accuracy of our method relative to several other FS techniques in a series of simulations and a case study of voter registration records in North Carolina. Finally, Section~\ref{sec:conclusion} discusses areas for further research.

%Crucially, these decisions are made independently for each pair. The \cite{fellegi_theory_1969} approach has been extended for a wide variety of applications \citep[e.g.,][]{Winkler1990, fair2004generalized, wagner2014person, gill2003english, enamorado_using_2019, aleshinguendel2021multifile}. 
%First, the original \cite{fellegi_theory_1969} method that models the linkage status of each record pair as independent typically results in one record in one file being matched with multiple records in the other file. This initial estimate of the linkage structure is usually refined through post-processing to create a bipartite matching \cite{jaro1989}. As of yet, this post-processing has not been adapted to estimate more general linkage structures, and the initial estimate without any post-processing is known to have poor precision.

%\cite{aleshin2023multifile} allowed for multiple matches within and across datafiles, but requires that the modeller explicitly set a maximum linkage cluster size, and models the dependency between records in such a way that limits scalability. 



\section{The Fellegi-Sunter Framework for Record Linkage}\label{sec:prior-work}

Consider two data files $A$ and $B$ comprising $n_A$ and $n_B$ records, respectively, and including $F$ linkage variables measured in both files. Let $[x] := \{1, \ldots, x\}$ denote the sequence of integers from 1 to $x$. For $i \in [n_A]$, let record $i$ be given by $A_i = (A_{i1}, \dots, A_{iF})$, so that $A = (A_i : i \in [n_A])$.  Similarly, for $j=1, \dots, n_B$, let record $j$ be given by $B_j = (B_{j1}, \dots, B_{jF})$, so that $B = (B_j : j \in [n_B])$. By convention, we label the two files such that $n_A \geq n_B$.

In record linkage tasks, records that refer to the same entity should be similar, and records that refer to different entities should be dissimilar. To represent this, \cite{fellegi_theory_1969} proposed the comparison vector $\gamma_{ij} = (\gamma_{ij}^1, \ldots, \gamma_{ij}^F),$ where $\gamma_{ij}^f$ is a comparison for field $f$ between records $A_i$ and $B_j$. Binary comparisons are commonly used due to their simplicity; partial agreement or more complicated comparisons can be used through similarity metrics or distance functions \citep{Winkler_1990, Bilenko_2003_adapt, elmagarmid_duplicate_2007}. We assume that each field's comparison is discretized, and we let $L_f$ denote the number of levels for field $f$. We collect all of the comparison vectors as $\bm{\Gamma}=\{\gamma_{ij}\}_{i=1,j=1}^{n_A,n_B}$.

Consider records from $A$ and $B$ as a disjoint set of nodes. We have an edge between two records if they are coreferent (or matching). Our parameter of interest is the collection of edges representing coreferent records. This parameter can be expressed in various ways depending on the model assumptions for a particular approach to record linkage. For example, \cite{fellegi_theory_1969} used a coreferent matrix $\Delta \in \{0, 1\}^{n_A \times n_B}$, where
\begin{align}\label{eqn:delta-notation}
	\Delta_{ij} =
	\begin{cases}
		1, & \text{if records}\;  A_i \; \text{and}\; B_j \; \text{refer to the same entity}; \\
		0, & \text{otherwise}.\\
	\end{cases}
\end{align}
This sparse matrix representation can become cumbersome for large linkage tasks. In the setting where each record in $B$ can match to at most one record in $A$, \cite{sadinle_bayesian_2017} proposed using the more compact vector $\bm{Z} = (Z_1, \ldots, Z_{n_B})$ for the records in $B$ such that
\begin{align*}%\label{eqn:z-notation}
	Z_{j} =
	\begin{cases}
		i, & \text{if records}\;  A_i \; \text{and}\; B_j  \; \text{refer to the same entity}; \\
		n_A + j, & \text{if record}\;  B_j \; \text{does not have a match in}\; A. \\
	\end{cases}
\end{align*}
When presenting the theory in this paper, we use either representation for convenience depending on the context. We can go back and forth between the two using $\Delta_{ij} = I(Z_j = i),$ where $I(\cdot) = 1$ when the expression inside the parentheses is true, and $I(\cdot) = 0$ otherwise.

\subsection{Models for Comparison Vector Based Record Linkage}\label{sec:model-review}

Record linkage models within the FS framework consist of a model for the comparison vectors $\bm{\Gamma}$, and a model for the linkage structure that classifies the record pairs as matches and non-matches. In all such methods, the model for the comparison vectors takes the form
%\begin{subequations}
	\begin{align*}
		&\gamma_{ij} \mid \Delta_{ij} = 1 \stackrel{iid}{\sim} \mathcal{M}(\bm{m}),  \\
		&\gamma_{ij} \mid \Delta_{ij} = 0  \stackrel{iid}{\sim} \mathcal{U}(\bm{u}).
	\end{align*}
%\end{subequations}
Here, $\mathcal{M}$ and $\mathcal{U}$ are the distributions for matching and non-matching record pairs, and $\bm{m}$ and $\bm{u}$ are their respective parameter values. When using comparison vectors with discrete agreement levels, $\mathcal{M}$ and $\mathcal{U}$ are collections of independent multinomial distributions for each linkage feature. Accordingly, $\bm{m} = (\bm{m}_1, \ldots, \bm{m}_F)$, where $\bm{m}_f = (m_{f1}, \ldots, m_{fL_f})$ and $m_{fl} = \mathbb{P}(\gamma_{ij}^f = l|\Delta_{ij} = 1)$ for all fields $f$ and agreement levels $l$. The $\bm{u}$ parameters are defined similarly, with $u_{fl} = \mathbb{P}(\gamma_{ij}^f = l|\Delta_{ij} = 0)$. 
%Throughout the rest of the paper, we use the likelihood function in  \eqref{eqn:likelihood}.

The original approach of \cite{fellegi_theory_1969} models the matching status of each record pair independently from all other record pairs. Accordingly, the authors model $\Delta_{ij}$ through independent, identically distributed Bernoulli random variables. We generally assume that data in the records is missing completely at random, and is thus ignorable \citep{little_statistical_2002}. Letting $I_{obs}(\cdot)$ denote an indicator of whether a record field is observed, the likelihood function after marginalizing out the missing data is
\begin{align}
	\label{eqn:likelihood}
	\mathcal{L}(\Delta, m,u\mid\bm{\Gamma})=\prod_{i=1}^{n_A}\prod_{j=1}^{n_B}\prod_{f=1}^F\prod_{l=1}^{L_f}\left[m_{fl}^{I(\Delta_{ij} = 1)}u_{fl}^{I(\Delta_{ij} =0)}\right] ^{I(\gamma_{ij}^{f} = l) I_{obs}(\gamma_{ij}^f)}.
\end{align}
The independence assumption at the level of the individual record pairs makes this model straightforward to fit through the EM algorithm \citep{sariyar_2010}. Recently, much work as been done to use this model for increasingly large linkage tasks, such as through the
\texttt{fastLink} package in \texttt{R} by \cite{enamorado_using_2019}, and the \texttt{splink} library in Python by \cite{Linacre_Lindsay_Manassis_Slade_Hepworth_2022}. However, the full independence assumption of the record pair matching tends to produce a large number of false matches, and post-processing is required to get a more accurate estimate of the linkage structure \cite{jaro_1989}.
 
Results tend to improve when prior knowledge about the linkage structure is incorporated into the model \cite{sadinle_bayesian_2017}. For example, there are many situations where practitioners know that there are no duplications within each file, and this can be incorporated into the model in a Bayesian framework through a prior distribution on the linkage structure. For this setting, \cite{sadinle_bayesian_2017} proposed the ``beta distribution for bipartite matching,'' given by
\begin{align}
	\label{eqn:brlprior}
	\mathbb{P}(\bm{Z}\mid \pi) &=\dfrac{[n_A-n_{12}(\bm{Z})]!}{n_A!} %\times
	\pi^{n_{12}(\bm{Z})}(1-\pi)^{n_B-n_{AB}(\bm{Z})},  	\\
	\pi &\sim \text{Beta}(\alpha_{\pi}, \beta_{\pi}), \notag
\end{align}
where $n_{AB} = \sum_{j=1}^{n_B} I(Z_j < n_A + j)$ is the number of links between files $A$ and $B$, and $\alpha_{\pi}$ and $\beta_{\pi}$ are fixed and known hyperparameters. This prior induces a Gibbs sampler that strictly enforces one-to-one matching, and has has been shown to outperform the base FS method when model assumptions are satisfied. This model is implemented in \texttt{R} through the \texttt{BRL} package, but has computational complexity that grows quadratically with the size of the linkage task, and is therefore limited to small to moderate data files \citep{BRL}.

More recently, \cite{aleshin2023multifile} extended the FS framework for the setting of arbitrary amount of duplications within and across an arbitrary number of files. To do so, they presented a generative process for multifile partitions, and parameterized each step of this generative process. In particular, they use a truncated Poisson distribution for the number of records associated with each unique entity exhibited in the linkage task, where the user designates an upper limit for the cluster size. Similarly the beta prior for bipartite matching, this Gibbs sampler as implemented in \texttt{multilink} becomes infeasible for larger linkage tasks. 

%This leads to a structured prior for multifile partitions consisting of
%\begin{enumerate}
%	\item A prior for the number of unique entities exhibited across files. By default, this is taken to be uniform from 1 to the total number of records in the linkage task.
%	\item Given the total number of unique entities, a prior on the number of unique entities exhibited in each subset of files. This prior is given by a multinomial-Dirichlet distribution.
%	\item Given the number of unique entities exhibited in each file, a prior for the number of records in each file associated with each unique entity. By default, this is taken to be a Poisson distribution truncated between 1 and a user specified upper bound.
%	\item Given the number of records in each file associated with each unique entity, a prior for the within-file partitions of records to entities. This is taken to be uniform over the labellings of such assignments.
%	\item Given the overlap table and within-file partitions, a prior for the matching between files. This is taken to be uniform over all possible such matchings. 
%\end{enumerate}
%This structured prior induces a Gibbs sampler on the partition of records corresponding to the unique entities in the record linkage task. Similarly the beta prior for bipartite matching however, this Gibbs sampler becomes infeasible for larger linkage tasks. 

%To our knowledge, this is as of yet the only comparison vector method that can identify duplicates within files with out employing the full record pair independence from the original \cite{fellegi_theory_1969}.
\subsubsection{Fast Beta Linkage}\label{sec:fabl}

\cite{kundinger_2023} proposed to model each element of $\bm{Z}$ independently, relaxing the assumption that there are no duplicates within each database. Under this relaxation, they introduced the fast beta linkage (\texttt{fabl}) prior:
\begin{align}\label{eqn:fabl}
	\mathbb{P}(Z_j = i \mid \pi) 
	&=
	\begin{cases}
		\pi/n_A,  &  \text{if}  \; i\leq n_A,\\
		1- \pi, &   \text{if}  \;  i= n_A + j,
	\end{cases} \\
	\pi &\sim \text{Beta}(\alpha_{\pi}, \beta_{\pi}).
\end{align}
This prior says that a record $B$ has some match in $A$ with probability $\pi$, and that each record in $A$ is equally likely to be that match. The \texttt{fabl} prior closely mimics the beta prior for bipartite matchings in \eqref{eqn:brlprior} without enforcing the bipartite matching restriction within the Gibbs sampler. This prior was originally proposed for efficient MCMC sampling, but we recently showed that this prior was conducive to variational inference. This implementation of the model, dubbed ``Variational Beta Linkage" (or \texttt{vabl}) was shown to conduct linkage 50-250 times fast than the original MCMC implementation in \texttt{fabl}\citep{kundinger_2024_vabl}. If desired, a bipartite matching can be acquired through a simple post-processing step after computing the Bayes estimate. 

\section{Dirichlet Record Linkage}\label{sec:drl}

%We showed in \cite{kundinger_2023} and \cite{kundinger_2024_vabl} that the \texttt{fabl} framework tends to outperform standard FS when each record in $B$ has at most one match in $A$. However, 

\brian{I edited this a bit for clarity. Do you think this should at the beginning or the end of this section? I like it at the beginning, to motivate the new material.}

Although the \texttt{fabl} framework is effective and efficient when $A$ has no internal duplications, significant problems arise when a record in $B$ has multiple matching records in $A$. In particular, posterior probability is normalized among all records in $A$, such that none of the matches achieves high enough posterior probability to be identified in the Bayes estimate. This amounts to a paradox: the more matches that record $B_j$ has in $A$, the less likely the algorithm is to find a match. Here, we attempt to resolve this paradox by extending the \texttt{fabl} framework to handle these internal duplications in $A$. We emphasize that we are not interested in deduplication within $A$ for its own sake, but rather aim to conduct record linkage in light the problems posed by these duplications. Therefore, duplications within files that have no duplications across files will go undetected; we believe this acceptable for many practical applications. 

We provide updated notation to describe matching in this setting. We drop the convention of labelling datasets such that $n_A \geq n_B$, and instead let $B$ denote the file that we assume is free of internal duplicates. In this setting, we refer to the duplicate free file $B$ as the reference file, and the other file $A$ as the target file. Let $Z_j = (Z_{j, 1}, \ldots)$ be a set containing the indices for all of the records in $A$ that are a match with record $B_j$, with $Z_j = \emptyset$ to denoting when $B_j$ has no match in $A$. Let $\bm{Z} = \{Z_j | j \in [n_B] \}$ denote the collection of all such sets for all records in $B$. Lastly, let $|Z_j|$ denote the number of records in $A$ that are linked to $B_j$. 

Define a vector of probabilities $\bm{\pi} = (\pi_0, \ldots)$ where $\pi_k$ is the probability that some record $B_j$ has exactly $k$ matches in $A$. To avoid setting a maximum number of matches for any $B_j$, we assign $\bm{\pi}$ a Dirichlet process prior. That is, 
\begin{align*}%\label{eqn:pi-dirichlet-process}
	f(\bm{\pi}) = \sum_{k=0}^{\infty} \pi_k \delta_{\pi_k}(\bm{\pi}),
\end{align*}
where $\delta_{\pi_k}$ is the indicator function which evaluates to zero everywhere, except for $\delta_{\pi_k}(\pi_k) = 1$. We model each $\pi_k$ as a product of conditional probabilities. Let $\bm{\eta} = (\eta_1, \ldots)$, and let $\eta_k$ be the probability that some record in $B$ has at least $k$ matches, given that it has at least $k-1$ matches. This gives us the stick breaking representation
\begin{align}\label{eqn:pi-stick-breaking}
	\pi_k = (1 - \eta_{k+1}) \prod_{c=1}^{k} \eta_c, 
\end{align}
where $\eta_k$ are independent random variables from a $\text{Beta}(\alpha_{\eta}, \beta_{\eta})$ distribution, for fixed and known hyperparameters $\alpha_{\eta}$ and  $\beta_{\eta}$. We use notation for an arbitrary $\alpha_{\eta}$ as it makes future derivations more readable, but we fix $\alpha_{\eta} = 1$ as is done in a typical Dirichlet process prior \citep{jordan_2006}. 

Conditional on $B_j$ having $k$ matches, we construct a prior specification on $\bm{Z}$ such that each matching $q$ of length $|q|$ is equally likely. Since there are ${n_A \choose |q|} = n_A!/ [(n_A - |q|)! |q|!]$ ways to select $|q|$ matching records out of all $n_A$ possible records, we let
\begin{align} \label{eqn:drl-prior}
	\mathbb{P}(Z_j = q| \bm{\pi}) = \frac{(n_A - |q|)! |q|!}{n_A!} \pi_{|q|}. 
	%\bm{\pi} \sim \text{Dirichlet}(\alpha_{\pi}) \label{eqn:pi_drl}.
\end{align}
In Appendix~\ref{app:joint-distribution}, we show that the full conditional distribution for $Z_j$ is given by
\begin{align} \label{eqn:joint_posterior}
		p\left(Z_j  = q|\bm{\Gamma}, \bm{m}, \bm{u}, \bm{\pi} \right) \propto \frac{(n_A - |q|)!|q|!}{n_A!} \pi_{|q|} \prod_{i \in q} w_{ij}, 
\end{align}
where for all $i \in [n_A]$ and $j \in [n_B]$, 
\begin{align*}
	%\label{eqn:fs_weight}
	w_{ij} = \prod_{f=1}^{F}\prod_{l = 1}^{L_f} \left(\frac{m_{fl}}{u_{fl}}\right)^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)} .
\end{align*}
Unfortunately, while the full conditional in \eqref{eqn:joint_posterior} has a closed form, sampling from it directly is infeasible for almost any record linkage task. In particular, there are $2^{n_A}$ possible matchings for each $B_j$, such that even enumerating these possible matchings would be computationally demanding. With MCMC however, we can sequentially sample individual components of $Z_j$ at a much lower computational cost.
%Unfortunately, while the full conditional in \eqref{eqn:joint_posterior} has a closed form, sampling from it directly is infeasible for almost any record linkage task. In particular, there are $2^{n_A}$ possible matchings for each $B_j$, such that even enumerating these possible matchings would be computationally demanding. One could reduce this set of possible matchings by setting a maximum number of matches, $K$, per record in $B$, but this would still require $\sum_{k = 1}^K \frac{n_A!}{(n_A - k)!k!}$ possible options for the set $Z_j$, and would be prohibitive for most values of $n_A$ seen in record linkage problems. With MCMC however, we can sequentially sample individual components of $Z_j$ at a much lower computational cost, and learn $K$ from the data, rather than setting it ahead of time. 

%In Section~\ref{sec:sequential-sampler}, we break the joint distribution of $Z_j$ into a sequence of more simple conditional univariate distributions. In Section~\ref{app:efficient-sampling}, we adapt methodology from \cite{kundinger_2023} to further reduce computational complexity. Additional details about initialization and posterior sampling for $\bm{m}$, $\bm{u}$, and $\bm{\eta}$ are provided in Appendix~\ref{app:gibbs-sampler}. In Section~\ref{sec:bayes-estimate}, we provide a loss function and decision rule to produce a point estimate of the linkage structure.

%\brian{Note discrepancy about power law?}

\subsection{Sampling $Z_j$} \label{sec:sequential-sampler} 

We use the stick breaking representation in \eqref{eqn:pi-stick-breaking} to generalize the fast beta prior in (\ref{eqn:fabl}), producing a sequence of priors that allows for each record in $B$ to match to multiple records in $A$. In each iteration of the Gibbs sampler, we sample an initial set of links using $\eta_1$. For each record in $B$ that was found to have a link, we remove the linked record in $A$ from consideration, and then sample another potential link with $\eta_2$. We continue, using $\eta_k$ in the $k^{th}$ matching step, until no new links are found, at which we point the matching phase terminates. Crucially, there is no need to specify a maximum number of links per record, as this is estimated through the model. 


We use the superscript $(s)$ to denote the iteration in the Gibbs sampler. Let $Z_{j, -k}^{(s)} = (Z_{j, 1}^{(s)}, \ldots, Z_{j, k-1}^{(s)})$ denote the indices for records in $A$ that have been matched to $B_j$ before matching step $k$ in Gibbs iteration $s$ (with $Z_{j, -k}^{(s)} = \emptyset$). Importantly, the size and ordering of $Z_j^{(s)}$ is random across Gibbs iterations. Let $D_k^{(s)} = \{j \in [n_B]| Z_{j, k}^{(s)} \neq \emptyset\}$ to denote the set of records in $B$ linked to at least $k$ records in $A$ in Gibbs iteration $s$ (with $D_0^{(s)} = [n_B]$).  

When $B_j$ has been linked to $k-1$ records, we use a prior that says that the probability that $B_j$ has a $k^{th}$ match is $\eta_k$, and that all $n_A - (k - 1)$ remaining records in $A$ are equally likely to be linked.  For each $j \in D_{k-1}^{(s)}$, we use
\begin{align} \label{eqn:sequential_prior}
	\mathbb{P}(Z_{j, k}^{(s+1)} = q_k|\eta_k, Z_{j, -k}^{(s+1)}) &= \begin{cases}
		\frac{\eta_k}{n_A - (k - 1)}, &  q_k \notin [n_A] \setminus Z_{j, -k}^{(s+1)}, \\
		1 - \eta_k, & q_k = \emptyset.
	\end{cases}
\end{align}
%where $A_{j, k}^{(s+1)} = [n_A] \setminus Z_{j, -k}^{(s+1)}$. 
This sequence of priors leads to sequence of posteriors that can be used to sample arbitrarily many links for record $B_j$. These full conditional distributions are given by 
\begin{align} \label{eqn:sequential_posterior}
	\mathbb{P}(Z_{j, k}^{(s+1)} = q_k|Z_{j, -k}^{(s+1)}, \eta_k, \bm{m}, \bm{u}, \bm{\Gamma}) &\propto \begin{cases}
		\frac{\eta_k}{n_A - (k - 1)} w_{q_k, j}, & q_k \in [n_A] \setminus Z_{j, -k}^{(s+1)}, \\
		1 - \eta_k, & q_k= \emptyset. \\
		%\emptyset, & Z_{j, k-1} = \emptyset.
	\end{cases}
\end{align}
We provide a derivation in Appendix~\ref{app:sequential-sampler}.

This sequential sampler produces an output $Z_j^{(s)} = q = (q_1, \ldots, q_k)$ when $Z_{j, c}^{(s)} = q_c$ for steps $c \in [k]$, and the $k+1$ step produces $Z_{j, k+1} = \emptyset$. While this vector is necessarily ordered, all reorderings of the same elements are equivalent for the purposes of record linkage.   That is, $Z_j^{(s)} = (i, i')$ and $Z_j^{(s)} = (i', i)$ both communicate that record $B_j$ is matched to records $A_i$ and $A_{i'}$. Let $\sigma(q)$ denote all $|q|!$ possible orderings of the elements of $q$. Marginalizing over such orderings, we have
\begin{align*}
	\mathbb{P}(Z_{j, k+1}^{(s)} = \emptyset | \bm{\Gamma}, \bm{m}, \bm{u}, \bm{\eta}) &\sum_{q' \in \sigma(q)} \prod_{c = 1}^{k} \mathbb{P}(Z_{j, c}^{(s)} = q'_c|\bm{\Gamma}, \bm{m}, \bm{u}, \bm{\eta}) \\
	&\propto (1 - \eta_{k+1}) \sum_{q' \in \sigma(q)} \prod_{c = 1}^{k} \frac{\eta_{c} }{n_A - (c - 1)}  \prod_{c = 1}^{k} w_{q'_c, j} \\
	&=(1 - \eta_{k+1}) k! \prod_{c = 1}^{k} \frac{\eta_{c} }{n_A - (c - 1)}  \prod_{c = 1}^{k} w_{q_c, j} \\
	&= \frac{(n_A - k)!k!}{n_A!} (1 - \eta_{k+1})\prod_{c = 1}^{k} \eta_{c} \prod_{c = 1}^{k} w_{q_c, j} \\
	&= \frac{(n_A - k)!k!}{n_A!} \pi_k \prod_{c = 1}^{k} w_{q_c, j} \\
	&= \mathbb{P}\left(Z_j^{(s)}  = q |\bm{\Gamma}, \bm{m}, \bm{u}, \bm{\pi}\right).
\end{align*}
Thus, the probability that the sequential process samples each of the components of $q$ (in any order) is equal to the joint probability of $q$ as expressed in the joint distribution in (\ref{eqn:joint_posterior}).

%$D_k^{(s)} = [n_{B}] \setminus \{j | Z_{j, k}^{(s)} = \emptyset\}$

\subsection{Summary of Gibbs Sampler}\label{sec:gibbs-sampler}

We initialize $\bm{Z}$ to reflect no matches across data files; that is, $Z_j = \emptyset$ for all $j \in [n_B]$. We then iteratively sample $\bm{m}$,  $\bm{u}$, $\bm{\eta}$, and $\bm{Z}$ for a prespecified number of Gibbs iterations. The full conditional for $Z_{j, k}^{(s)}$ is provided in Appendix~\ref{app:sequential-sampler}, the full conditional for $\bm{\eta}^{(s)}$ is provided in Appendix~\ref{app:sample-eta}, and full conditionals for $\bm{m}^{(s)}$ and $\bm{u}^{(s)}$ are provided in Appendix~\ref{app:derive-m-u}. We summarize this sampler in Algorithm~\ref{alg:gibbs-sampler}.

%A computationally efficient method to sample $\bm{Z}$ using the hashing described in Appendix~\ref{app:hashing} is provided in Appendix~\ref{app:efficient-sampling}. 

%Algorithm~\ref{alg:gibbs-sampler} provides a complete overview of the Gibbs sampler used in \texttt{DRL}. We provide derivations of the full conditional distributions for $\bm{m}$ and $\bm{u}$ in Appendix~\ref{app:derive-m-u}, and derivations for $\bm{\eta}$ in Appendix~\ref{app:sample-eta}. 

%We use the notation $D_k^{(s)} = \{j \in [n_B]| Z_{j, k}^{(s)} \neq \emptyset\}$ to denote the set of records in $B$ for which the algorithm attempts to find matches in matching step $k$ in Gibbs iteration $s$ (with $D_0^{(s)} = [n_B]$).

%Define $B_k = \{j \in [n_B]| Z_{j, k-1} \neq \emptyset\}$ with $B_1 = [n_B]$. At stage $k$ of the matching step, the sampler samples a potential match for each record in $B_k$. The matching step terminates at step $k^{(*)}$ when $B_{k*} = \emptyset$.

%\SetKwComment{Comment}{/* }{ */}
%\RestyleAlgo{ruled}
%\begin{algorithm}
%	\caption{Gibbs Sampler for Dirichlet Record Linkage}\label{alg:gibbs-sampler}
%	\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
%	\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
%	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}\SetKwInOut{Initialize}{Initialize}
%	\Input{Data $\bm{\Gamma}$ and hyperparameters $\bm{\alpha}, \bm{\beta},$ and $\bm{\beta}_{\eta}$} 
%	%\Output{$\hat{\Delta}, \hat{\bm{m}}, \hat{\bm{u}}, \hat{\bm{\eta}}$}
%	\Initialize{$Z_j = \emptyset, \forall j \in [n_B]$}
%	\For{$s \in [S]$}{
%
%		Sample $m_{fl}^{(s+1)} \sim \mathbb{P}(m_{fl} | \bm{\Gamma}, \bm{Z}^{(s)},  \bm{u}^{(s)}, \bm{\eta}^{(s)})$
%		
%		Sample $u_{fl}^{(s+1)} \sim \mathbb{P}(m_{fl} | \bm{\Gamma}, \bm{Z}^{(s)},  \bm{m}^{(s+1)}, \bm{\eta}^{(s)})$
%		
%		Set $k = 1$
%		
%		\While{$D_{k-1}^{(s+1)} \neq \emptyset$}{
%			
%		Sample $\eta_{k}^{(s+1)} \sim \mathbb{P}(\eta_k | \bm{\Gamma}, \bm{Z}^{(s)},  \bm{m}^{(s+1)}, \bm{u}^{(s +1)})$
%		
%		\For{$j \in D_{k-1}^{(s+1)}$}{
%		Sample $Z_{j, k}^{(s+1)} \sim \mathbb{P}(Z_{j, k} | Z_{j, -k}^{(s+1)}, \eta_k^{(s+1)}, \bm{\Gamma},  \bm{m}^{(s+1)}, \bm{u}^{(s+1)})$
%	}
%	Set $k = k + 1$
%	}
%}
%	\Return{$\{\bm{Z}^{(s)}, \bm{m}^{(s)}, \bm{u}^{(s)}, \bm{\eta}^{(s)}\}_{s=1}^S$}
%\end{algorithm}

\begin{algorithm}
	\caption{Gibbs Sampler for Dirichlet Record Linkage}\label{alg:gibbs-sampler}
	\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
	\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}\SetKwInOut{Initialize}{Initialize}
	\Input{Data $\bm{\Gamma}$ and hyperparameters $\bm{\alpha}, \bm{\beta},$ and $\bm{\beta}_{\eta}$} 
	%\Output{$\hat{\Delta}, \hat{\bm{m}}, \hat{\bm{u}}, \hat{\bm{\eta}}$}
	\Initialize{$Z_j = \emptyset, \forall j \in [n_B]$}
	\For{$s \in [S]$}{
		\For{$f \in [F]$}{
				Sample $\bm{m}_f^{(s+1)} \sim \text{Dirichlet}\left(\alpha_{f1}(\bm{Z}^{(s)}), \ldots, \alpha_{fL_f}(\bm{Z}^{(s)})\right)$, where 
				
				\hskip2.0em $\alpha_{fl}(\bm{Z}^{(s)}) = \alpha_{fl} + \sum_{i=1}^{n_A}\sum_{j=1}^{n_B} I_{obs}(\gamma_{ij}^f) I(\gamma_{ij}^f = l) I(i \in Z_j^{(s)})$
				
				Sample $\bm{u}_f^{(s+1)} \sim \text{Dirichlet}\left(\beta_{f1}(\bm{Z}^{(s)}), \ldots, \beta_{fL_f}(\bm{Z}^{(s)})\right)$
				
				\hskip2.0em $\beta_{fl}(\bm{Z}^{(s)}) = \beta_{fl} + \sum_{i=1}^{n_A}\sum_{j=1}^{n_B} I_{obs}(\gamma_{ij}^f) I(\gamma_{ij}^f = l) I(i \notin Z_j^{(s)})$
				}
		
		Set $k = 1$
		
		\While{$D_{k-1}^{(s+1)} \neq \emptyset$}{
			
			Sample $\eta_{k}^{(s+1)} \sim \text{Beta}\left(n_{k}^{(s)} + \alpha_{\eta}, n_{k-1}^{(s)} - n_{k}^{(s)} + \beta_{\eta} \right)$, where
			
			\hskip2.0em $n_k^{(s)} = \sum_{j=1}^{n_B} I(|Z_j^{(s)}| \geq k)$
			
			\For{$j \in D_{k-1}^{(s+1)}$}{
				Sample $Z_{j, k}^{(s+1)} \propto \begin{cases}
					\frac{\eta_k^{(s+1)}}{n_A - (k - 1)} w_{q_k, j}^{(s+1)}, & q_k \in [n_A] \setminus Z_{j, -k}^{(s+1)}, \\
					1 - \eta_k^{(s+1)}, & q_k= \emptyset.
				\end{cases}$
			}
			Set $k = k + 1$
		}
	}
	\Return{$\{\bm{Z}^{(s)}, \bm{m}^{(s)}, \bm{u}^{(s)}, \bm{\eta}^{(s)}\}_{s=1}^S$}
\end{algorithm}

%\alpha_{fl} + \sum_{i=1}^{n_A}\sum_{j=1}^{n_B} I_{obs}(\gamma_{ij}^f) I(\gamma_{ij}^f = l) I(i \in Z_j)

We take a moment to emphasize the computational advantages to this approach. Sampling from the full conditional shown in (\ref{eqn:joint_posterior}) has complexity $O(2^{n_A}n_B)$, and would be nearly impossible for any reasonable record linkage task. Using the sequential sampler shown in (\ref{eqn:sequential_posterior}) has complexity $O\left(n_A \sum_{k=1}^{n_A} n_{k-1}^{(s)}\right)$, where $n_k^{(s)} = |D_{k}^{(s)}| = \sum_{j=1}^{n_B} I\left(|Z_j^{(s)}| \geq k\right)$ is the number of records in $B$ linked to at least $k$ records in $A$ in Gibbs iteration $s$. 

Even with the sequential sampler however, this complexity would still grow quadratically as the size of the linkage task grows. Therefore, we follow the methodology from \cite{kundinger_2023} to conduct sampling at the level of the unique instances of $\gamma_{ij}$ in the linkage task, instead of all $n_A n_B$ comparison vectors, to greatly reduce computational complexity. We review this methodology in Appendix~\ref{app:hashing}, and provide an adaptation of \eqref{eqn:sequential_posterior} using this methodology in Appendix~\ref{app:efficient-sampling}. Using this approach, the sampler has complexity $O\left(P \sum_{k=1}^{n_A} n_{k-1}^{(s)}\right)$, which grows linearly in the size of the base dataset. Lastly, we note since records in $B$ generally have a small number duplicates in $A$, most terms in the summation  $\sum_{k=1}^{n_A} n_{k-1}^{(s)}$ are zero. Thus, we have produced a computationally efficient sampler with complexity comparable to the $O(P n_B)$ complexity of \texttt{fabl}. 

\brian{Beka commented that computational complexity should be ``proven." This doesn't really seem like something to be proved though. In particular, there is no way to really relate the true amount of duplications $n_k$ to the number of duplicatations found in Gibbs iterations $n_k^{(s)}$, which is a random quantity (and generally greater that $n_k$). Its more a description than a proof.}

%However, sampling from (\ref{eqn:gibbs1}) and (\ref{eqn:gibbs2}) has complexity $O\left(P (n_B + \sum_{k=1}^K n_{k-1}^{(s)})\right)$, which grows linearly in the size of the base dataset. Thus, we have produced a computationally efficient sampler with speed comparable to the $O(P n_B)$ complexity of \texttt{fabl}.

%This sequential sampler amounts to an extension of \texttt{fabl} with an iterative matching phase. See Appendix~\ref{app:gibbs-sampler} for a full derivation and explanation of the Gibbs sampler.





\subsection{Bayes Estimate}\label{sec:bayes-estimate} 

After generating MCMC samples through Algorithm~\ref{alg:gibbs-sampler}, we require a loss function to produce a point estimate of the linkage structure. Unfortunately, many of the loss functions that have been proposed for different record linkage models are not amenable to \texttt{DRL}. For example, \cite{sadinle_bayesian_2017} proposed a loss function that ensured a Bayes estimate that respected one-to-one matching and allowed certain components of the estimate to remain undeclared and left for clerical review. However, this loss function relies on making exactly one linkage decision per record in $B$, and is not amenable to our scenario where we do not set a maximum number of links per record. More recently, \cite{aleshin2023multifile} proposed a loss function that estimated a partition, and thus respected transitivity, and also allowed for leaving parts of the Bayes estimate undeclared. The implementation of this loss function however relies on each MCMC sample itself being a partition, which does not hold true in our relaxed setting. 

For this reason, we instead use a generalization of the absolute number of errors loss function from \cite{Tancredi_2011}. It is more convenient here to use the matrix representation of the linkage structure $\Delta$ as in \eqref{eqn:delta-notation}, rather than the $\bm{Z}$ notation used throughout Section~\ref{sec:drl}. Specifically, we seek an estimate $\hat{\Delta}$ of the linkage structure $\Delta$ that minimizes the loss function
\begin{align*}%\label{eqn:loss function}
	L(\Delta, \hat{\Delta}) = \sum_{i = 1}^{n_A}\sum_{j = 1}^{n_B}\tau \hat{\Delta}_{ij}(1 - \Delta_{ij}) - \Delta_{ij}(1 - \hat{\Delta}_{ij}),
\end{align*}
where $\tau > 0 $ is a penalization term specified by the user to weigh the importance of false matches relative to false non-matches. This loss function is equivalent to 
\begin{align*}
	L(\Delta, \hat{\Delta}) = \sum_{i = 1}^{n_A}\sum_{j = 1}^{n_B}\tau \hat{\Delta}_{ij} - (1 + \tau)\hat{\Delta}_{ij}\Delta_{ij} + \Delta_{ij}.
\end{align*}
Minimizing $L(\Delta, \hat{\Delta})$ is equivalent to maximizing the quantity
\begin{align*}
	W(\hat{\Delta}) = \hat{\Delta}_{ij}[(1 + \tau) \Delta_{ij} - \tau].
\end{align*}
The expectation of this quantity with respect to the linkage structure is given by 
\begin{align*}
	E[W(\hat{\Delta})] =\hat{\Delta}_{ij}[(1 + \tau) \mathbb{P}(\Delta_{ij} = 1) - \tau],
\end{align*}
which is maximized by the decision rule
\begin{align}\label{eqn:decision-rule}
	\hat{\Delta}_{ij} = \begin{cases}
		1,  &  \text{if}  \; \mathbb{P}(\Delta_{ij} = 1) > \frac{\tau}{1 + \tau},\\
		0 &   \text{otherwise}.
	\end{cases}
\end{align}
When the user specifies $\tau = 1$, indicating that false matches and false non-matches are weighted equally, we recover the decision rule of \cite{Tancredi_2011} with a threshold of 0.5 to declare matches. This decision rule also functions similarly to that of \cite{sadinle_bayesian_2017} under certain choices of the losses. 

Since the \texttt{DRL} prior as specified in in \eqref{eqn:drl-prior} does not strictly enforce the requirement that there are no duplicates in $B$, it is possible for this Bayes estimate to link multiple records in $B$ to the same record in $A$. To obtain a Bayes estimate that corresponds to our model assumptions, we minimize the expected loss subject to the constraint that $\hat{Z}_{j, k} \neq \hat{Z}_{j', k}$ for all $j \neq j'$ and all $k$. This means that when multiple records in $B$ are declared as matching to the same record in $A$ in the initial Bayes estimate, we simply accept the one with highest posterior probability, and declare all other pairs as non-matching. We make the simplifying assumption that all records that match to the same $B_j$ also match to each other, but this is not verified in the model. We believe this assumption is well suited for many practical applications, and is worth the computational advantages of the \texttt{DRL} approach.

\section{Simulations}\label{sec:simulations}

We demonstrate the accuracy of \texttt{DRL} through an adaptation of the simulation study from \cite{aleshin2023multifile}. We compare the performance of \texttt{DRL} against models implemented in \texttt{fastLink}, \texttt{vabl}, and \texttt{multilink}.  \texttt{BRL} and \texttt{fabl} provide similar accuracy to \texttt{vabl}, and are therefore omitted. 

For each simulation, we construct two data files $A$ and $B$ of size $n_A = n_B = 500$. File $B$ is constructed to have no internal duplicates, and we designate 10\% of the records in $B$ (or 50 records) to have some number of matches in $A$. The number of matches these records have in $A$ is sampled from a Poisson distribution truncated to the interval [1, 5]. We use Poisson distributions with means 0.1, 1, and 2, and denote these settings as low, medium, and high duplication settings respectively. Each of these duplicate records has up to three altered fields relative to the original record in $A$. After accounting for the records in $A$ that have a match in $B$, the rest of the records in $A$ are comprised of unique entities. We follow the methodology of \cite{christen_vatsalan2013} to generate original records for the unique entities by simulating first name, last name, age, postal code, and occupation from frequency tables based on demographic information in Australia. Fields are then chosen for distortion uniformly at random. Names are subject to string insertions, deletions and substitutions, as well as common keyboard, phonetic, and optical recognition errors. Age and occupation are distorted through keyboard errors and missingness.

We create comparison vectors for these linkage tasks according to the thresholds described in Table~\ref{Tab:sadinle_simulation_cutoffs}. We use uniform priors for $\bm{m}$ and $\bm{u}$, with $\alpha_{fl} = \beta_{fl} = 1$ for all $f$ and $l$. We use uniform priors for $\pi$ for \texttt{vabl}, and also uniform priors for each $\eta_k$ in the sequential sampler for \texttt{DRL}. For \texttt{multilink}, we correctly designate that there are no duplicates in $B$ and that $A$ can have up to 5 records that are linked together (and to one record in $B$). We use the default setting of a Poisson distribution with mean 1 for the number of records clustered together in $A$. In consultation with the authors of \texttt{multilink}, we also implement the method with filtering, requiring that both first and last name have an agreement level of 1 or 2 in order for the record pair to be considered for matching. For \texttt{DRL} and \texttt{multilink}, we use a Gibbs sampler with 1000 iterations and discard the first 100 as burn-in. For \texttt{fastLink} and \texttt{vabl}, we run the algorithm with a convergence tolerance of 1e-04, in line with default settings. \texttt{fastLink} results are provided without the Jaro correction that enforces one-to-one matching. 

\begin{table}
	\centering
	\begin{tabular}{lllll}
		\multicolumn{2}{c}{ } & \multicolumn{3}{c}{Level of Disagreement} \\
		\cline{3-5}
		Fields & Similarity & 1 & 2 & 3 \\
		\hline
		First and Last Name & Levenstein & 0 & (0, .25] &  (.5, .1]\\
		Age, Postal Code, and Occupation & Binary & Agree & Disagree &  \\
		\hline
	\end{tabular}
	\caption{Construction of comparison vectors for accuracy study with simulated data files of Section`\ref{sec:simulations}.}
	\label{Tab:sadinle_simulation_cutoffs}
\end{table}

We compute point estimates of the linkage structure using comparable decision rules for each method. For each Bayesian method, we use losses reflecting equal importance for false matches and false non-matches. For \texttt{vabl} and \texttt{DRL}, this results in a probability threshold of 0.5 to declare record pairs as a match. We choose this threshold for \texttt{fastLink} as well. For each of these, we obtain an initial point estimate, then post-process so that no two records in $B$ match to same record in $A$. For \texttt{multilink}, \cite{aleshin2023multifile} do not provide closed form expressions for the decision rule for their Bayes estimate, but we select analogous losses. 

To compare the accuracy of each method, we use recall, precision, and F-measure as defined by \cite{christen_data_2012}. Recall is the proportion of true matches found by the estimate; given by
$$\hat{\theta}_{\text{Recall}} = \frac{\sum_{i=1}^{n_A}\sum_{j=1}^{n_B} \Delta_{ij} \hat{\Delta}_{ij}}{\sum_{i=1}^{n_A}\sum_{j=1}^{n_B} \Delta_{ij}}.$$
%$$\theta_{\text{Recall}} = \frac{\sum_{i=1}^{n_A}\sum_{j=1}^{n_B} I(\Delta_{ij} = 1)I(\hat{\Delta}_{ij} = 1)}{\sum_{i=1}^{n_A}\sum_{j=1}^{n_B} I(\Delta_{ij} = 1)}.$$
Precision is the proportion of links found by the estimate that are true matches, given by
%$$\theta_{\text{Precision}} = \frac{\sum_{i=1}^{n_A}\sum_{j=1}^{n_B}I(\Delta_{ij} = 1)}{\sum_{i=1}^{n_A}\sum_{j=1}^{n_B} I(\hat{\Delta}_{ij} = 1)}.$$
$$\hat{\theta}_{\text{Precision}} = \frac{\sum_{i=1}^{n_A}\sum_{j=1}^{n_B}\Delta_{ij}\hat{\Delta}_{ij}}{\sum_{i=1}^{n_A}\sum_{j=1}^{n_B} \hat{\Delta}_{ij}}.$$
Lastly, we compute F-measure, a pseudo-average of recall and precision, given by 
\begin{align*}
	\hat{\theta}_{\text{F-measure}} = 2 \left(\frac{\hat{\theta}_{\text{Recall}} \times \hat{\theta}_{\text{Precision}}}{\hat{\theta}_{\text{Recall}} +  \hat{\theta}_{\text{Precision}}}\right).
\end{align*}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{../figures/poisson_fig1.png}
	\caption{Results from simulation study of described in Section~\ref{sec:simulations} for \texttt{vabl}, \texttt{fastLink}, and \texttt{DRL}. For each method and simulation setting, dots show the median, and bars show the $2.5^{\text{th}}$ and $97.5^{\text{th}}$ percentiles of recall and precision of Bayes estimates across 100 simulated datasets.}
	\label{fig:sim-vabl-comparison}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{../figures/poisson_fig2.png}
	\caption{Results from simulation study of described in Section~\ref{sec:simulations} for \texttt{multilink} (with and without filtering) and \texttt{DRL}.}
	\label{fig:sim-multilink-comparison}
\end{figure}

In Figure~\ref{fig:sim-vabl-comparison}, we see that in the low duplication setting, \texttt{vabl} performs comparatively with \texttt{DRL} in terms of recall and precision. However, as the number of internal duplications in $A$ grows, recall suffers. This is because \texttt{vabl} is constrained to identify at most one match for each record in $B$, and in many cases, posterior probability is split among several truly matching record pairs such that none of them reach the threshold to be declared a match. In contrast, \texttt{fastLink} maintains higher recall at all duplication settings, and precision improves as the rate of duplication grows. Upon closer analysis, the actual number of false positive matches remains similar across the duplication settings, but the larger number of true positives results in higher precision. 

The results from \texttt{multilink} require a more nuanced interpretation. While \texttt{DRL} only evaluates the record pairs between each record in $A$ and one particular $B_j$, \texttt{multilink} must also consider the comparison vectors between record pairs in $A$. Notably, each record in $A$ can have up to three errors relative to the original record in $B$, which means that records in $A$ can have up to six errors relative to each other. This means that \texttt{multilink} is engaging in a more difficult matching process, and in this sense, is not directly comparable to \texttt{DRL}. With that caveat mentioned, we see in Figure~\ref{fig:sim-multilink-comparison} that \texttt{multilink} without filter has poor precision under these simulation settings. With filtering however, the precision greatly improves. Nonetheless, as the rate of duplication increases, recalls suffers for both implementations of \texttt{multilink}. This because 5\% to 10\% of truly matching record pairs get removed from consideration by our chosen filtering scheme, and because of the difficulty of matching large clusters as described above.

In contrast, \texttt{DRL} maintains strong performance at all duplication rates considered. Notably, \texttt{DRL} has only slightly poorer precision than \texttt{vabl} even in the low duplication setting where the model assumptions of \texttt{vabl} are closest to being satisfied. Additionally, \texttt{DRL} is consistently able to match records in $B$ to several records in $A$ without needing to set a maximum number of links through the prior distribution, as is needed in \texttt{multilink}. 

We note that \texttt{vabl} is incredibly fast, reaching convergence in around 0.01 seconds on average. Due to the design of the \texttt{fastLink} package in \texttt{R}, we cannot directly measure the speed of the EM algorithm separately from the construction of the comparison vectors, but we assume that it is even faster than \texttt{vabl}. When we use filtering, \texttt{multilink} completes 1000 iterations of the Gibbs sampler in about 0.5 seconds. Without filtering however, it takes around 1000 seconds. Lastly, \texttt{DRL} completes the 1000 Gibbs iterations in about 15 seconds. 

In Figure~\ref{fig:sim-time} we see that \texttt{fabl} completes in an average of 10 seconds at all duplications levels in the simulation. In contrast, computation time for \texttt{DRL} increases as the duplication level grows from an average of 10 seconds at the low level to 15 seconds at the highest level. We note that \texttt{vabl} is incredibly fast, reaching convergence in around 0.01 seconds on average. For this reason, we recommend \texttt{vabl} in cases where it is known that records in $B$ match to at most record in $A$, and recommend \texttt{DRL} when the duplications within $A$ makes \texttt{vabl} unreliable. 

Due to the design of the \texttt{fastLink} package in \texttt{R}, we cannot directly measure the speed of the EM algorithm separately from the construction of the comparison vectors, but we assume that it is even faster than \texttt{vabl}.


We note that this is only slightly slower than \texttt{fabl}, and clearly not approaching the upper bound on the complexity as shown in Lemma~\ref{lemma:complexity}.

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{../figures/poisson_time.png}
	\caption{Computation time from simulation study of described in Section~\ref{sec:simulations} for \texttt{fabl} and \texttt{DRL}.}
	\label{fig:sim-time}
\end{figure}

We conducted another simulation with 30\% of records in $B$ having matches in $A$, with all other settings held constant. Under these settings, \texttt{fastLink} and \texttt{multilink} perform better, but \texttt{DRL} still outperforms. We include these results in Appendix~\ref{app:simulation-2}.



\section{Case Study}\label{sec:case-study}

We demonstrate our method on two snapshots of the North Carolina Voter Registration (\texttt{NCVR}) database taken two months apart ~\citep{christen_preparation_2014}. The snapshots are filtered to include only those voters whose details changed over the two-month period, so there are no matching records with full agreement on all fields. We use first name, middle name, last name, as string fields, and street address and age as categorical fields. Unique voter registration numbers are provided, however they are known to contain some errors. The \texttt{NCVR} dataset is not publicly available due to sensitive information. However, we have permission to utilize it for publication by its owner.

Using the voter registration numbers, we can see that each file has internal duplication rates of about 1\%. In this analysis, we deduplicate file $B$, and leave $A$ with duplicates. In practice, such low amount of internal duplication may not warrant the use of \texttt{DRL} since \texttt{vabl} is considerably faster. However, we demonstrate here that even with such few internal duplications, \texttt{DRL} is effective at identifying these multiple matches and does not declare too many false matches. 

We compare four approaches: \texttt{vabl}, \texttt{DRL}, \texttt{fastLink}, and \texttt{fastLink} with the Jaro correction. This task is too large for running \texttt{multilink} in its current implementation in \texttt{R}, and is thus omitted. We use a threshold of 0.5 to declare matches, and post-process so no two records in $B$ match to the same record in $A$, as done in Section~\ref{sec:simulations}. Results are in Table \ref{table:ncvr_results}.

\begin{table}[t]
	\centering
	\begin{tabular}{l|rrr}
		Method & Recall & Precision & F-measure\\
		\hline
		\texttt{DRL} & 0.9891623 & 0.9735309 & 0.9812843\\
		\hline
		\texttt{vabl} & 0.9743739 & 0.9798799 & 0.9771191\\
		%\hline
		%fabl\_swap & 0.9842840 & 0.9420737 & 0.9627164\\
		\hline
		\texttt{fastLink} & 0.9988472 & 0.8494865 & 0.9181321\\
		\hline
		\texttt{fastLink} (with Jaro) & 0.9514490 & 0.9664229 & 0.9588775\\
	\end{tabular}
	\caption{Accuracy results for \texttt{NCVR}, showing the \texttt{DRL} has the highest F-measure.}
	\label{table:ncvr_results}
\end{table}

We see that the initial \texttt{fastLink} estimate without one-to-one post-processing results in the highest recall, but leads to an undesirable amount of false positives. Since the posterior match probability of each record pair is computed independently, we expect such behavior on large linkage tasks such as this. When we use the methodology of \cite{jaro_1989} to achieve a one-to-one matching, we get considerably better results. However, we lose the possibility of identifying cases where one record in $B$ matches to two records in $A$, and we still have worse recall and precision than attained under \texttt{vabl}. 

In Figure~\ref{fig:ncvr_thresholds}, we show the precision, recall, and F-measure under various choices of match probability thresholds, corresponding to different choices for $\tau$ in the decision rule in \eqref{eqn:decision-rule}. We see that \texttt{vabl} and \texttt{DRL} produce more refined match probability estimates, allowing the accuracy metrics to vary smoothly across the different thresholds. In contrast, under \texttt{fastLink}, all record pairs of the same agreement pattern have the same posterior probability, leading to much more coarse posterior distribution and more erratic curves. We see that \texttt{vabl} maintains the highest precision at all thresholds, but that \texttt{DRL} is able to identify enough more additional true matches that \texttt{DRL} maintains the highest overall F-measure.
%Overall, we see that \texttt{DRL} has the highest F-measure under all probability thresholds considered, followed by \texttt{fabl}, and then by \texttt{fastLink}.
\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\textwidth]{../figures/ncvr_thresholds}
	\caption{Accuracy metrics for NCVR data at various match probability thresholds. We see that \texttt{DRL} maintains the strongest F-measure for all thresholds considered.}
	\label{fig:ncvr_thresholds}
\end{figure}

%We are specifically interested in how \texttt{DRL} performs with the regards to the records in $B$ that have multiple matching records in $A$. We filter $\Delta$ and $\hat{\Delta}$ to examine only the record pairs associated with multiple matches. Specifically, we produce
%\begin{align*}
%	\Delta_{dup} = \left\{\Delta_{ij} \Big| j \in \left\{\sum_{i = 1}^{n_A} \Delta_{ij} > 1 \right\}  \right\} \\
%	\text{and } \hat{\Delta}_{dup} = \left\{\hat{\Delta}_{ij} \Big| j \in \left\{\sum_{i = 1}^{n_A} \hat{\Delta}_{ij} > 1 \right\}  \right\} .
%\end{align*}
%In this analysis, $\Delta_{dup}$ shows matches associated with 1811 unique records $B_j$, exhibited in 3619 record pairs. At the 0.5 probability threshold, $\hat{\Delta}_{dup}$ shows matches associated with 2339 unique records $B_j$, exhibited in 3619 record pairs


%There are 1805 records in $B$ that have 2 matches in $A$, and 3 records in $B$ and have 3 matches in $A$, resulting in 3622 record pairs associated with multiple matches. At the 0.5 probability threshold, \texttt{DRL} correctly identifies 89.9\% of these matches. Additionally, 98.8\% of the declared matches associated with multiple matches are correct. 

%At the 0.5 probability threshold, \texttt{DRL} identifies 3258 record pairs that are associated with multiple matches.

%\texttt{DRL} correctly identifies 89.9\% of these matches. Additionally, 98.8\% of the declared matches 

Lastly, we note a limitation in using preexisting software for record linkage tasks. In its currently implementation, \texttt{fastLink} allows users to define normalized Levenshtein distance thresholds for coding field comparisons into three discrete levels. In this data however, it is common for a record to contain a middle initial (like ``E"), rather than a full middle name (like ``Elizabeth"). Using Levenshtein distance thresholds, the comparison field for middle name for such a pair of records could be coded as a full disagreement, when it should be more reasonably given its own agreement level. Likewise, when two records have a middle initial that happens to match, this is coded as a full agreement, even though it is possible these initials represent different names. In this analysis, 27.6\% of the false matches under \texttt{DRL} at the 0.5 probability threshold included at least one record containing a middle initial rather than middle name. A more careful construction of the comparison vectors is likely to improve results for all methods considered.

\section{Conclusion}\label{sec:conclusion}

In this paper, we have introduced Dirichlet Record Linkage, a new Bayesian method for matching one duplicate free reference file to another file that may have duplicates. We have discussed the trade offs between model complexity and speed across several comparison vector based methods. If a practitioner is sure that each file has no (or reasonably few) internal duplicates, then \texttt{vabl} allows for high accuracy and low computation time. If a practitioner is interested in deduplicating both datasets simultaneously, the additional computation of \texttt{multilink} may be worth the ability to needs to more carefully model the partition structure of the data. When a practitioner is confident that one dataset is relatively duplicate free, \texttt{DRL} offers an intermediate method that is robust to internal duplications in the other file while maintaining reasonable speed. 

In future work, we seek to adapt the methodology presented here to more closely approximate the model implemented by \texttt{multilink}. At current, \texttt{DRL} either post-processes a Bayes estimate such that there are no implicit matchings between two records in the ``duplicate-free" data file, or refrains from post-processing altogether, leaving a Bayes estimate that may not respect transitive closure. While we have argued this acceptable for many practical purposes, we acknowledge that it is not acceptable for others. Additionally, while the MCMC sampler derived in this paper was shown to scale to sizeable linkage tasks, we also acknowledge the inherent scalability limitations of MCMC methods. We hope to explore the utility of variational inference in addressing these limitations(as in \cite{kundinger_2024_vabl}), and develop new methods for the challenges of modeling the joint distribution of clusters of matching records of varying sizes. 

\pagebreak
\bibliographystyle{agsm}
\bibliography{vabl}

\pagebreak

\section{Appendix}
\label{sec:appendix}

\subsection{Full Conditional for the Set $Z_j$}\label{app:joint-distribution}

We express the likelihood in \eqref{eqn:likelihood} for the model assumptions of \texttt{DRL} as
\begin{align*} %\label{eqn:drl-likelihood}
	\mathcal{L}(\bm{Z}, \bm{m}, \bm{u} \mid \bm{\Gamma})=\prod_{i=1}^{n_A}\prod_{j=1}^{n_B}\prod_{f=1}^F\prod_{l=1}^{L_f}\left[m_{fl}^{I(i \in Z_j)}u_{fl}^{I(i \notin Z_j)}\right] ^{I(\gamma_{ij}^{f} = l) I_{obs}(\gamma_{ij}^f)}.
\end{align*}
Let $\Gamma_{.j} = \{\gamma_{ij} | i \in [n_A]\}$ be the collection of $n_A$ comparison vectors associated with record $B_j$. We follow the proof technique introduced by \cite{wortman2019} and elaborated by \cite{kundinger_2023}. When $B_j$ does not link to any record in $A$ (such that $|Z_j| = 0$) the contribution to the likelihood is simply a product of $u$ parameters, which we will call $c_j$. 
\begin{align*}
	\mathbb{P}(\Gamma_{.j}| \bm{m}, \bm{u}, \bm{\pi}, Z_j = \emptyset) = \prod_{i=1}^{n_A}\prod_{f=1}^{F}\prod_{l=1}^{L_f} u_{fl}^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)} = c_j.
\end{align*}
When $Z_j = q =  (q_1, \ldots, q_k)$ for some $|q| > 0$, we have
\begin{align*}
	\mathbb{P}(\Gamma_{.j}| \bm{m}, \bm{u}, \bm{\pi},  Z_j = q) =\prod_{i \in q}\prod_{f=1}^{F}\prod_{l=1}^{L_f} m_{fl}^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)}  \prod_{i \notin q}\prod_{f=1}^{F}\prod_{l=1}^{L_f} u_{fl}^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)}.
\end{align*}
We multiply and divide by the $u$ parameters for the matching record pairs to obtain
\begin{align*}
	\mathbb{P}(\Gamma_{.j}| \bm{m}, \bm{u}, \bm{\pi}, Z_j = q) &= \prod_{i \in q}\prod_{f=1}^{F}\prod_{l=1}^{L_f} \left(\frac{m_{fl}}{u_{fl}}\right)^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)}  \prod_{i = 1}^{n_A}\prod_{f=1}^{F}\prod_{l=1}^{L_f} u_{fl}^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)} \\
	&= c_j \prod_{i \in q} w_{ij} .
\end{align*}
Lastly, we multiply the likelihood by the prior in (\ref{eqn:drl-prior}) to obtain the posterior distribution. We have
%\begin{subequations}
	\begin{align*}
		p\left(Z_j  = q|\bm{\Gamma}, \bm{m}, \bm{u}, \bm{\pi} \right) &= \frac{\frac{(n_A - k)!|k|!}{n_A!} \pi_{k} c_j \prod_{i \in q} w_{ij}}{\sum_{h \in \mathcal{Z}} \frac{(n_A - |h|)!|h|!}{n_A!} \pi_{|h|} c_j \prod_{i \in h} w_{ij}} \\
		&= \frac{\frac{(n_A - k)!|k|!}{n_A!} \pi_{k} \prod_{i \in q} w_{ij}}{\sum_{h \in \mathcal{Z}} \frac{(n_A - |h|)!|h|!}{n_A!} \pi_{|h|} \prod_{i \in h} w_{ij}} \\
		&\propto \frac{(n_A - k)!|k|!}{n_A!} \pi_{k} \prod_{i \in q} w_{ij}.
	\end{align*}
%\end{subequations}
Importantly, the constant $c_j$ is not found in the final expression because the probability mass associated with every potential value for $Z_j$ shares the same $c_j$. This does not occur due to proportionality.

\subsection{Full Conditional for Component $Z_{j, k}$} \label{app:sequential-sampler}
We now provide the derivation of the sequential sampler, following the argument presented in Section \ref{app:joint-distribution}. Suppose $B_j$ has been linked to $k$ records in $A$. Recall that $Z_{j, -k}^{(s)} = (Z_{j, 1}^{(s)}, \ldots, Z_{j, k-1}^{(s)})$ denotes the vector of records already linked to $B_j$ in Gibbs iteration $s$. When $B_j$ has no additional link in $A$, the contribution to the likelihood is a product of the $u$ parameters for all remaining records. That is, 
\begin{align*}
	\mathbb{P}(\Gamma_{.j}| \bm{m}, \bm{u}, \bm{\pi}, Z_{j, k}^{(s)} = \emptyset, Z_{j, -k}^{(s)} = q_{-k}) = \prod_{i \notin q_{-k}} \prod_{f=1}^{F}\prod_{l=1}^{L_f} u_{fl}^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)} = c_{Z_{j, -k}^{(s)}}.
\end{align*}
When $Z_{j, k}^{(s)} = q_k$ for some $q_k > 0$, we have
\begin{align*}
	&\mathbb{P}(\Gamma_{.j}| \bm{m}, \bm{u}, \bm{\pi},  Z_{j, k}^{(s)} = q_k, Z_{j, -k}^{(s)} = q_{-k}) \\ &=\prod_{f=1}^{F}\prod_{l=1}^{L_f} m_{fl}^{I(\gamma_{q_k, j}^f = l)I_{obs}(\gamma_{q_k, j}^f)}  \prod_{i \notin (q_{-k}, q_k)}\prod_{f=1}^{F}\prod_{l=1}^{L_f} u_{fl}^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)} \\
	&=\prod_{f=1}^{F}\prod_{l=1}^{L_f} \left(\frac{m_{fl}}{u_{fl}} \right)^{I(\gamma_{q_k, j}^f = l)I_{obs}(\gamma_{q_k, j}^f)}  \prod_{i \notin q_{-k}}\prod_{f=1}^{F}\prod_{l=1}^{L_f} u_{fl}^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)} \\
	&= c_{Z_{j, -k}^{(s)}} w_{q_k, j}
\end{align*}
To obtain the posterior, we multiply by the prior in (\ref{eqn:sequential_prior}). The posterior distribution this is given by
\begin{align*} %\label{eqn:sequential-posterior-full}
	\mathbb{P}&(Z_{j, k}^{(s)} = q_k|Z_{j, k-1}^{(s)}, \eta_k, \bm{m}, \bm{u}, \gamma) \\
	&= \frac{\left(\frac{\eta_k}{n_A - (k - 1)}c_{Z_{j, -k}^{(s)}} w_{q_k, j}\right)^{I(q_k \in [n_A] \setminus Z_{j, -k}^{(s)})} + \left(c_{Z_{j, -k}^{(s)}}(1 - \eta_k)\right)^{I(q_k = \emptyset)}}{\frac{\eta_k}{n_A - (k - 1)}c_{Z_{j, -k}^{(s)}} \sum_{i \notin Z_{j, -k}} w_{ij} + c_{Z_{j, -k}}(1 - \eta_k)} \\
	&=
	\frac{\left(\frac{\eta_k}{n_A - (k - 1)}w_{q_k, j}\right)^{I(q_k \in [n_A] \setminus Z_{j, -k}^{(s)})} + (1 - \eta_k)^{I(q_k = \emptyset)}}{\frac{\eta_k}{n_A - (k - 1)}\sum_{i \notin Z_{j, -k}} w_{ij} +(1 - \eta_k)} \\
	&\propto \begin{cases}
		\frac{\eta_k}{n_A - (k - 1)} w_{q_k, j}, & q_k \in [n_A] \setminus Z_{j, -k}^{(s)}, \\
		1 - \eta_k, & q_k= \emptyset.
	\end{cases}
\end{align*}
Importantly, the constant $c_{Z_{j, -k}}^{(s)}$ is not found in the final expression because the probability mass associated with every potential value for $Z_{j, k}^{(s)}$ shares the same $c_{Z_{j, -k}}^{(s)}$. This does not occur due to proportionality. 


\subsection{Hashing for Efficient Posterior Inference}\label{app:hashing}
Following \cite{kundinger_2023} we use hashing to reduce the computational complexity of the Gibbs sampler. Since each component $\gamma_{ij}^f$ of the comparison vector is discrete, there are only finitely many possible realizations of the comparison vector $\gamma_{ij}$. Let $P$ be the number of unique agreement patterns observed in $\bm{\gamma}$. This number is bounded above by $P^{*} =  \prod_{f=1}^F (L_f + 1)$, where the addition of 1 to $L_f$ for each field accounts for the possibility of missing values. This upper bound does not scale with $n_A$ or $n_B$, but rather is determined by $F$ and $L_f$.  

We adopt a hashing function to map the $P$ unique agreement patterns present in the comparison vectors to the integers in $[P]$. Let
\begin{equation*}
	%\label{eq:hashfunc}
	h_f^{(i,j)} = I_{obs}( \gamma_{ij}^f) 2^{\gamma_{ij}^f + I(f>1) \times \sum_{e=1}^{f-1}(L_{e} - 1)}
\end{equation*}
denote a hashed value for field $f$ for record pair $(i,j)$. Summing over the fields for the agreement pattern of record pair $(i,j)$ gives the hash value $h^{(i,j)}=\sum_{f=1}^Fh_f^{(i,j)}$. We enumerate the unique hashed agreement patterns from $1$ to $P$. Denote each agreement pattern as $h_p=(h_p^1,\dots,h_p^F)$, so that when record pair $(i,j)$ exhibits agreement pattern $p$, we write $\gamma_{ij}=h_p$.  Collect the agreement patterns as $\mathcal{P}=\{h_p\mid p\in[P]\}$.

Let $e(h_p)$ denote the $\sum_{f=1}^FL_f$ length vector in which the $l + \sum_{k=1}^{f-1} L_k$ component is 1 when $h_p^f = l$, and 0 otherwise. Observe that $e(h_p)$ represents a long version of $h_p$, which is useful for computational purposes. Let $N_{p_j}=\sum_{i=1}^{n_B}I(\gamma_{ij}=h_p)$ denote the number of records in $A$ with which record $j$ in $B$ has agreement pattern $p$. Collect these counts in $\mathcal{N}=\{N_{p_j}\mid j\in[n_B], p\in[P]\}.$ Let $N_p=\sum_{j=1}^{n_B}N_{p_j}$ denote the total number of record pairs with agreement pattern $p$. Finally, let $r_{p_j}=\{i\in[n_A]\mid \gamma_{ij}=h_p\}$ be the set of records in $A$ with which record $j$ in $B$ has agreement pattern $p$, and collect these sets as $\mathcal{R}=\{r_{p_j}\mid p\in[P], j\in[n_B]\}$. 

The set $\tilde{\bm{\Gamma}}=\{ \mathcal{P}, \mathcal{R},\mathcal{N}\}$ fully characterizes the information in $\bm{\Gamma}$ at a reduced storage cost. In particular, by letting 
\[
m_p = \prod_{f=1}^F\prod_{l=1}^{L_f} m_{fl}^{I(h_p^{f}=l)I_{obs}(h_p^{f})}
\]
and
\[
u_p = \prod_{f=1}^F\prod_{l=1}^{L_f} u_{fl}^{I(h_p^{f}=l)I_{obs}(h_p^{f})},
\]
the likelihood function in \eqref{eqn:likelihood} can be written as
\begin{equation}
	\label{eq:likelihood-hash}
	\mathcal{L}(\bm{Z}, m,u\mid\tilde{\Gamma})=\prod_{j=1}^{n_B}\prod_{p=1}^{P}\prod_{i\in r_{p_j}}m_p^{I(i \in Z_j)}u_{p}^{I(i \notin Z_j)}.
\end{equation}
Thus this hashing procedure allows for exact inference on the original model, without any approximation.

\subsubsection{Batched Computation of Comparison Vectors} \label{app:batching}
When it becomes infeasible to store all $n_A  n_B$ comparison vectors at one time, we partition $A$ and $B$ into $t_1$ and $t_2$ smaller disjoint batches, as proposed by \cite{kundinger_2023}. Let $\{A^1, \dots, A^{t_1}\}$ be a partition of $A$ such that 
$$\cup_{a=1}^{t_1} A^a = A \quad \text{and} \quad A^a \cap A^{a'}=\emptyset \quad \text{for all} \quad a \neq a'.$$  Likewise, let $\{B^1, \dots, B^{t_2}\}$ be a partition of $B$ such that 
$$\cup_{b=1}^{t_2} B^b = B \quad \text{and} \quad B^b \cap B^{b'}=\emptyset \quad \text{for all} \quad b \neq b'.$$ For each $a$ and $b$, we compute comparison vectors for all records in $A^a \times B^b$, creating the comparison matrix $\bm{\Gamma}^{ab}$. 

We then perform hashing on the batch of comparison data, obtain a compressed $\tilde{\bm{\Gamma}}^{ab}$, and delete the memory intensive matrix $\bm{\Gamma}^{ab}$ before continuing with the next batch of data. That is, we calculate
\begin{align*}
	r_{p_j}^{ab} &= \{i \in [n_A]\mid \gamma_{ij} = h_p, i\in A^a, j \in B^b\}, \\
	N_{p_j}^{ab} &= |r_{p_j}^{ab}|.
\end{align*}
%where $r_{p_j}^{ab}$ is needed to compute point estimates as outlined in Appendix \ref{app:bayes-estimate}.
We compute these quantities for each batch in parallel. Finally, summary statistics from each pairwise batch comparison are combined to recover summary statistics for the full comparison matrix $\bm{\Gamma}$ through
\begin{align}
	r_{p_j} &= \{r_{p_j}^{ab}\}_{a=1, b=1}^{t_1,t_2} \label{eq:rpj}  \\
	\text{and } N_{p_j} &= \sum_{a = 1}^{t_1} \sum_{b = 1}^{t_2} N_{p_j}^{ab}. \notag
\end{align}

We can further reduce memory costs through storage efficient indexing (SEI) \citep{kundinger_2023}. All records $i$ in $A$ that share agreement pattern $p$ with record $B_j$ have the same weight $w_{p}$ (as described in Appendix \ref{app:hashing}). These records have the same probability to be identified as the link for record $B_j$. Thus, records $i \in r_{p_j}$ with large $N_{p_j}$ are unlikely to be sampled consistently enough to be deemed a match in the Bayes estimate. Rather than store all of these record labels, we store only a small number $S$. For each $r_{p_j}^{ab}$, we sample $S$ indices without replacement to form SEI$(r_{p_j}^{ab})$. We collect these memory-reduced lists to form SEI$(r_{p_j})$ as in (\ref{eq:rpj}), and collect these to form SEI$(\mathcal{R})$.

Lastly, when using SEI along with batching in this manner, we implicitly restrict the sequential sampler described in Section~\ref{sec:sequential-sampler}. If $|\text{SEI}(r_{p_j})| < N_{p_j}$ after conducting SEI, there is a non-zero probability we can sample an agreement pattern in \eqref{eqn:gibbs1} and not have a corresponding record label for it when sampling in \eqref{eqn:gibbs2}. Therefore, when using SEI, we must limit the amount of sequential sampling steps to some number $K < S$.

\subsection{Gibbs Sampler}\label{app:gibbs-sampler}

We initialize $\bm{Z}$ to reflect no matches across data files; that is, $Z_j = \emptyset$ for all $j \in [n_B]$. We then iteratively sample $\bm{m}$,  $\bm{u}$, $\bm{\eta}$, and $\bm{Z}$ for a prespecified number of Gibbs iterations. A computationally efficient method to sample $\bm{Z}$ using the hashing described in Appendix~\ref{app:hashing} is provided in Appendix~\ref{app:efficient-sampling}. The the full conditional for $\bm{\eta}$ is provided in Appendix~\ref{app:sample-eta}, and full conditionals for $\bm{m}$ and $\bm{u}$ are provided in Appendix~\ref{app:derive-m-u}.

\subsubsection{Efficient Sequential Sampling for Components $Z_{j, k}$}\label{app:efficient-sampling}

Let $\tilde{\bm{\Gamma}}=\{ \mathcal{P}, \mathcal{R},\mathcal{N}\}$ denote the set of summary statistics described in Appendix~\ref{app:hashing}. We provide a computationally efficient implementation of the sampler in \eqref{eqn:sequential_posterior} by using, and updating, quantities in $\mathcal{R}$ and $\mathcal{N}$ as the sequential sampler progresses. Let $r_{p_j}(k) = r_{p_j} \ (Z_{j, 1}, \ldots Z_{j, k-1})$ be the set of records of agreement pattern $p$ available for matching at the $k^{th}$ matching step, and let $N_{p_j}(k) = |r_{p_j}(k)|$ be the number of such records. We first sample among $P + 1$ options for the agreement pattern between $B_j$ and its potential link. Define $r$ as an arbitrary set of records. We have
\begin{align}
	\label{eqn:gibbs1}
	p\left( Z_{j, k} \in r \mid \tilde{\bm{\Gamma}}, \bm{m}, \bm{u}, \eta_k \right) \propto
	\begin{cases} 
		\frac{\eta_k N_{p_j}(k)}{n_A - (k - 1)}  w_{p},  & r = r_{p_j}(k); \\
		1- \eta_k , &  r = \emptyset. \\
	\end{cases}
\end{align}
Since all remaining records in $A$ sharing the same agreement pattern with $B_j$ are equally likely, we then sample among candidate records uniformly using
\begin{align}
	\label{eqn:gibbs2}
	p\left(Z_{j, k} = q \mid Z_{j, k} \in r, \bm{m}, \bm{u}, \eta_k \right) = \begin{cases} 
		\frac{1}{N_{p_j}(k)}, & r = r_{p_j}(k) \text{ and } q \in r; \\
		1, & r = \emptyset \text{ and } q = \emptyset. \\
	\end{cases}
\end{align}
Sampling from \eqref{eqn:gibbs1} and \eqref{eqn:gibbs2} is mathematically equivalent to sampling from the full conditional for $Z_{j,k}$ in \eqref{eqn:sequential_posterior}.

%We take a moment to emphasize the computational advantages to this approach. Sampling from the full conditional shown in (\ref{eqn:joint_posterior}) would have complexity $O(2^{n_A}n_B)$, and would be nearly impossible for any reasonable record linkage task. Using the sequential sampler shown in (\ref{eqn:sequential_posterior}) in Gibbs iteration $s$ has complexity $O\left(n_A (n_B + \sum_{k=1}^K n_{k-1}^{(s)})\right)$, where $n_k^{(s)} = \sum_{j=1}^{n_B} I\left(|Z_j^{(s)}| \geq k\right)$ is the number of records in $B$ linked to at least $k$ records in $A$. Even with the sequential sampler, this complexity  would still grow quadratically as the size of the linkage task grows. However, sampling from (\ref{eqn:gibbs1}) and (\ref{eqn:gibbs2}) has complexity $O\left(P (n_B + \sum_{k=1}^K n_{k-1}^{(s)})\right)$, which grows linearly in the size of the base dataset. Thus, we have produced a computationally efficient sampler with speed comparable to the $O(P n_B)$ complexity of \texttt{fabl}.

%The full conditionals for $\bm{m}$ and $\bm{u}$ are similar to those shown in \cite{kundinger_2023}, and are thus moved to Appendix~\ref{app:derive-m-u}. Full conditionals for $Z_j$ and $\bm{\eta}$ are provided in Sections~\ref{sec:sampling-zjk} and \ref{sec:sampling-zjk} respectively below. 

\subsubsection{Sampling Match Rate $\eta_k$}\label{app:sample-eta}

Define $n_k = \sum_{j=1}^{n_B} I\left(|Z_j| \geq k\right)$ to be the number of records in $B$ that are linked to at least $k$ records in $A$ (with $n_0 = n_B$). Note that step $k$ of the sequential sampler acts on $n_{k-1}$ records in $B$ and identifies $n_k$ links. As such, the full conditional for $\eta_k$ is a standard Beta-Binomial update. For each $\eta_k$, we have
\begin{align*}
	%\label{eqn:eta-full-conditional}
	&\mathbb{P}\left(\eta_k \big| \bm{\Gamma}, \bm{Z}, \bm{m}, \bm{u}\right) \\
	&\propto \mathbb{P}\left(\bm{Z}| \eta_k\right) \mathbb{P}(\eta_k) \\
	&=\prod_{j=1}^{n_B} \mathbb{P}\left(Z_{j} | \eta_k\right) \mathbb{P}(\eta_k) \\
	&\propto \prod_{j=1}^{n_B} \left[\eta_k^{I\left(|Z_j| \geq k \right)} (1 - \eta_k)^{1 - I\left(|Z_j| \geq k \right)}\right]^{I\left(|Z_j| \geq k-1 \right)} \eta_k^{\alpha_{\eta} - 1}(1 - \eta_k)^{\beta_{\eta} - 1} \\
	&\propto \eta_k^{n_{k} + \alpha_{\eta} - 1} (1 - \eta_k)^{n_{k-1} - n_{k} + \beta_{\eta} - 1}.
\end{align*}
Thus we have
\begin{align*}
	\eta_k |\bm{Z}, \bm{m}, \bm{u}, \bm{\Gamma} \sim \text{Beta}\left(n_{k} + \alpha_{\eta}, n_{k-1} - n_{k} + \beta_{\eta} \right).
\end{align*}

\subsubsection{Full Conditional for $m$ and $u$}\label{app:derive-m-u}

The $\bm{m}$ and $\bm{u}$ parameters are updated through standard multinomial-Dirichlet distributions. For a particular $m_{fl}$, we have
\begin{align*}
	\mathcal{L}(m_{fl}|\bm{\Gamma}, \bm{u}, \bm{Z}, \bm{\eta}) &\propto \prod_{i=1}^{n_A} \prod_{j=1}^{n_B} m_{fl}^{I(i \in Z_j) I(\gamma_{ij}^f = l) I_{obs}(\gamma_{ij}^f)}  m_{fl}^{\alpha_{fl} - 1} = m_{fl}^{\alpha_{fl}(\bm{Z}) - 1},
\end{align*}
where $\alpha_{fl}(\bm{Z})= \alpha_{fl} + \sum_{i=1}^{n_A}  \sum_{j=1}^{n_B} I_{obs}(\gamma_{ij}^f)I(\gamma_{ij}^f = l) I(i \in Z_j)$. Analogous procedures lead to $\mathcal{L}(u_{fl}| \bm{\Gamma}, \bm{m}, \bm{Z}, \bm{\eta})$  $\propto u_{fl}^{\beta_{fl}(\bm{Z}) - 1}$, where $\beta_{fl}(\bm{Z})= \beta_{fl} + \sum_{i=1}^{n_A}  \sum_{j=1}^{n_B} I_{obs}(\gamma_{ij}^f)I(\gamma_{ij}^f = l) I(i \notin Z_j)$. Thus, for the vectors of parameters $\bm{m}_f$ and $\bm{u}_f$, we have
%\begin{subequations}
	\begin{align}
		\bm{m}_f|\bm{\Gamma}, \bm{Z}, \bm{u}, \bm{\eta} &\sim \text{Dirichlet}(\alpha_{f1}(\bm{Z}), \ldots, \alpha_{fL_f}(\bm{Z}), \notag \\
		\bm{u}_f|\bm{\Gamma}, \bm{Z}, \bm{m}, \bm{\eta} &\sim \text{Dirichlet}(\beta_{f1}(\bm{Z}), \ldots, \beta_{fL_f}(\bm{Z})), \notag \\
		\text{where }\alpha_{fl}(\bm{Z})&= \alpha_{fl} + \sum_{i=1}^{n_A}\sum_{j=1}^{n_B} I_{obs}(\gamma_{ij}^f) I(\gamma_{ij}^f = l) I(i \in Z_j), \label{eqn:alpha_update} \\
		\text{and } \beta_{fl}(\bm{Z})&=  \beta_{fl} + \sum_{i=1}^{n_A}\sum_{j=1}^{n_B}  I_{obs}(\gamma_{ij}^f) I(\gamma_{ij}^f = l) I(i \notin Z_j) \label{eqn:beta_update}.
	\end{align}
%\end{subequations}

Using the hashing techniques provided in Appendix~\ref{app:hashing} and likelihood as expressed in \eqref{eq:likelihood-hash}, we can perform these updates in a more efficient manner. Let $\bm{\alpha_0} = (\alpha_{11}, \ldots, \alpha_{F L_F})$ and $\bm{\beta_0}= (\beta_{11}, \ldots, \beta_{F L_F})$ be concatenated vectors of prior parameters for the $\bm{m}$ and $\bm{u}$ distributions respectively. The terms needed for the posterior updates for the $\bm{m}$ and $\bm{u}$ parameters are given by the appropriate components of the vectors
%\begin{subequations}
	\begin{align}
		\bm{\alpha(\bm{Z})} &= \bm{\alpha_0} + \sum_{p=1}^P n_p\left(\bm{Z} \right) \times e(h_p), \label{efficient_alpha} \\
		\bm{\beta(\bm{Z})} &= \bm{\beta_0} + \sum_{p=1}^P \left(N_p - n_p\left(\bm{Z}\right)\right) \times e(h_p), \label{efficient_beta}
	\end{align}
%\end{subequations}
where $n_p(\bm{Z}) = \sum_{i = 1}^{n_A}\sum_{j = 1}^{n_B} I(i \in Z_{j}) I\left(\gamma_{ij} = h_p \right)$ is the number of matching record pairs with agreement pattern $h_p$. Specifically, the $l + \sum_{k=1}^{f-1} L_k$ components of (\ref{efficient_alpha}) and (\ref{efficient_beta}) provide the posterior updates for level $l$ and field $f$ in (\ref{eqn:alpha_update}) and (\ref{eqn:beta_update}). These vectorized summations can be computed more efficiently than summing over $n_A n_B$ record pairs for each field and agreement level.

\subsection{Summary of Efficient Gibbs Sampler with Hashing} \label{app:efficient-algorithm}

\begin{algorithm}
	\caption{Efficient Gibbs Sampler for Dirichlet Record Linkage}\label{alg:gibbs-sampler-efficient}
	\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
	\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}\SetKwInOut{Initialize}{Initialize}
	\Input{Data $\tilde{\bm{\Gamma}}$ and hyperparameters $\bm{\alpha}, \bm{\beta},$ and $\bm{\beta}_{\eta}$} 
	%\Output{$\hat{\Delta}, \hat{\bm{m}}, \hat{\bm{u}}, \hat{\bm{\eta}}$}
	\Initialize{$Z_j = \emptyset, \forall j \in [n_B]$}
	\For{$s \in [S]$}{
		\For{$f \in [F]$}{
			Sample $\bm{m}_f^{(s+1)} \sim \text{Dirichlet}\left(\alpha_{f1}(\bm{Z}^{(s)}), \ldots, \alpha_{fL_f}(\bm{Z}^{(s)})\right)$, where 
			
			\hskip1.0em $\alpha_{fl}(\bm{Z}^{(s)})$ is component $l + \sum_{k=1}^{f-1} L_k$ of the sum $\alpha_{fl} +  \sum_{p=1}^P n_p\left(\bm{Z}^{(s)} \right) \times e(h_p)$
			
			Sample $\bm{u}_f^{(s+1)} \sim \text{Dirichlet}\left(\beta_{f1}(\bm{Z}^{(s)}), \ldots, \beta_{fL_f}(\bm{Z}^{(s)})\right)$, where
			
			\hskip1.0em $\beta_{fl}(\bm{Z}^{(s)})$ is component $l + \sum_{k=1}^{f-1} L_k$ of the sum $\beta_{fl} +  \sum_{p=1}^P \left(N_p - n_p\left(\bm{Z}^{(s)} \right)\right) \times e(h_p)$
		}
		
		Set $k = 1$
		
		\While{$D_{k-1}^{(s+1)} \neq \emptyset$}{
			
			Sample $\eta_{k}^{(s+1)} \sim \text{Beta}\left(n_{k}^{(s)} + \alpha_{\eta}, n_{k-1}^{(s)} - n_{k}^{(s)} + \beta_{\eta} \right)$, where
			
			\hskip2.0em $n_k^{(s)} = \sum_{j=1}^{n_B} I(|Z_j^{(s)}| \geq k)$
			
			\For{$j \in D_{k-1}^{(s+1)}$}{
				Sample $r^{(s+1)} \propto \begin{cases} 
					\frac{\eta_k^{(s+1)} N_{p_j}{(s+1)}(k)}{n_A - (k - 1)}  w_{p}^{(s+1)},  & r^{(s+1)} = r_{p_j}^{(s+1)}(k); \\
					1- \eta_k^{(s+1)} , &  r^{(s+1)} = \emptyset. \\
				\end{cases}$
			
				Sample $Z_{j, k}^{(s+1)} \propto \begin{cases} 
					\frac{1}{N_{p_j}^{(s+1)}(k)}, & r^{(s+1)} = r_{p_j}^{(s+1)}(k) \text{ and } q \in r^{(s+1)}; \\
					1, & r^{(s+1)} = \emptyset \text{ and } q = \emptyset. \\
				\end{cases}$
			}
		
			Set $k = k + 1$
		}
	}
	\Return{$\{\bm{Z}^{(s)}, \bm{m}^{(s)}, \bm{u}^{(s)}, \bm{\eta}^{(s)}\}_{s=1}^S$}
\end{algorithm}

\subsection{Computational Complexity}

In this Appendix, we prove the computation complexity for \texttt{DRL}. 

\begin{lemma}\label{lemma:complexity}
		Let $n_A$ and $n_B$ be the number of records in files $A$ and $B$, respectively. Let $F$ be the number of fields used for comparisons across records, and $P$ be the number of patterns that comparison vectors exhibit in $A \times B$. We assume $C$ cores available for parallelization and a Gibbs sampler with $T$ iterations. Then, the overall computational complexity of \texttt{fabl} with hashing is $O(F n_A n_B /C) + O(T P n_B^2/ C)$.
\end{lemma}
		%\begin{proof}
	
		We consider two steps: constructing the comparison vectors and the Gibbs sampler. The computational complexity of all pairwise comparisons across $A$ and $B$ is $O(F n_A n_B)$. The hashing procedure for all pairwise comparisons is also $O(F n_A n_B)$. With $C$ processors available, we can split these computations across $C$ equally sized partitions and compute these comparisons in parallel, so the complexity becomes $O(F n_A n_B /C)$. There are then trivial computational costs associated with synthesizing summary statistics across these partitions. We note that this contribution to computational complexity applies for all versions of \cite{fellegi_theory_1969} algorithms unless they use blocking to reduce the comparison space.
			
			Without hashing, the computational complexity of updating the $\bm{m}$ and $\bm{u}$ parameters is $O(F n_A n_B)$. However, by doing calculations over the agreement patterns rather than the individual records, hashing reduces the overall complexity to $O(P)$. The complexity of updating $\bm{Z}$ sequentially at the record level as in \cite{sadinle_bayesian_2017} is $O(n_A n_B)$. With hashing, we first sample the agreement pattern of the match with complexity $O(n_B P)$, and then we sample the record exhibiting that pattern with complexity $O(n_B)$. Thus, the complexity of sampling $\bm{Z}$ in a single iteration is $O(n_B P)$. Since $P << n_A$ in most applications, we have reduced the complexity of sampling $\bm{Z}$ from $O(F n_A n_B)$ under \texttt{BRL} to $O(n_B P)$ under \texttt{fabl}. With parallelization, this complexity is further reduced to $O(n_B P /C)$, and so the entire Gibbs sampler has complexity $O(T n_B P / C).$
			In summary, the total computational complexity for \texttt{fabl} is $O(F n_A n_B / C) + O(T n_B P / C).$
			%\end{proof}

\subsection{Additional Simulation}\label{app:simulation-2}

We repeat the simulation from Section~\ref{sec:simulations} with a larger proportion of the records in $B$ having matches in $A$. Specifically, we increase the number of such records to 150, rather than 50. As shown in Figures~\ref{fig:sim-vabl-comparison-2} and \ref{fig:sim-multilink-comparison-2}, the precision for \texttt{multilink} without filtering and for \texttt{fastLink} is much better in this setting, but still is not competitive with \texttt{DRL}. 

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{../figures/poisson_fig1_app.png}
	\caption{Results from simulation study of described in Appendix~\ref{app:simulation-2} for \texttt{vabl}, \texttt{fastLink}, and \texttt{DRL}.}
	\label{fig:sim-vabl-comparison-2}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{../figures/poisson_fig2_app.png}
	\caption{Results from simulation study of described in Appendix~\ref{app:simulation-2} for \texttt{multilink} (with and without filtering) and \texttt{DRL}.}
	\label{fig:sim-multilink-comparison-2}
\end{figure}

\subsection{Simulation Traceplots}\label{app:simulation-traceplots}


Describe behavior of $\eta$ traceplot.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\textwidth]{../figures/poisson_total_match_trace.png}
	\caption{Traceplot for total number of matches identified under \texttt{DRL} for sample linkage task in simulation described in Section~\ref{sec:simulations}.}
	\label{fig:total-match-traceplot}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\textwidth]{../figures/poisson_eta_trace.png}
	\caption{Traceplot for $\bm{\eta}$ under \texttt{DRL} for sample linkage task in simulation described in Section~\ref{sec:simulations}.}
	\label{fig:eta-traceplot}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\textwidth]{../figures/poisson_m_trace.png}
	\caption{Traceplot for $\bm{m}$ under \texttt{DRL} for sample linkage task in simulation described in Section~\ref{sec:simulations}.}
	\label{fig:m-traceplot}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\textwidth]{../figures/poisson_u_trace.png}
	\caption{Traceplot for $\bm{u}$ under \texttt{DRL} for sample linkage task in simulation described in Section~\ref{sec:simulations}.}
	\label{fig:u-traceplot}
\end{figure}

\end{document}

%In Figure~\ref{fig:sim-multilink-comparison}, we see that \texttt{multilink} without filtering tends to have both poor recall and precision in this setting. With filtering, \texttt{multilink} achieves comparable performance to \texttt{DRL} at the low duplication setting. 

%In these simulations, standard \texttt{fabl} drastically underperforms, so much so we omit results from the figures below. For each record in $B$ with matching records in $A$, the posterior match probability is split between the two matching records. Due to the randomness of the Gibbs sampler, one of the records occasionally has a posterior probability over 0.5 and is thus identified through the Bayes estimate, but often, both records have posterior probability below 0.5. Under \texttt{vabl}, the posterior probability of the two records is cut precisely in half, and thus \texttt{vabl} did not identify any matching record pairs in any of the simulations. Again, these results are omitted.

%We see that \texttt{fastLink} generally has poorer performance than the more complex models at mid to high overlap. In particular, without a post-processing step tailors the scenario where one record in $B$ can match to multiple records in $A$, \texttt{fastLink} tends to produce a large number of false positives, hindering precision, as shown in Figure \ref{fig:sadinle-precision}.

%The performance of \texttt{multilink} is more varied. When using substantive prior information, \texttt{multilink} outperforms \texttt{DRL} in terms of overall F-measure in the medium and high overlap settings. However, when the number of matching records pairs is low, \texttt{multilink} underperforms. This may be because the model being fit is more complex, and so it is more difficult to learn the parameters from such few observations. Notably however, the performance of \texttt{multilink} is highly sensitive to prior specification. When using default settings, \texttt{multilink} showed considerably lower recall and precision at all levels of error and overlap.



%The "fabl swap" approach here is also strong. It generally has higher recall, but lower precision. This makes sense, because it doesn't need to consecutive matching algorithim in order to find the "multiple matches", but it also doesn't employ any post processing to clean up erroneous matches. In all simulations, multiple match and fabl swap have similar f-measure. (Not sure exactly how I want to analyze that. In NCVR, there is more of a difference.) 

%We see that \texttt{DRL} is a strong alternative to \texttt{multilink} and \texttt{fastLink} in this setting. \texttt{DRL} maintains strong performance even in the low overlap scenario where \texttt{multilink} fails, and outperforms \texttt{fastLink} in terms of F-measure at all levels of error and overlap. We accomplish this through default, uniform priors for the sequence of $\eta_k$ parameters, without needing the informative priors required for \texttt{multilink}. Additionally, the average computation time for 1000 iterations of the Gibbs sampler for \texttt{DRL} was around 30 seconds, while it was around 1000 seconds for \texttt{multilink}. 
